import findspark
findspark.init()

import pyspark
import sys
import re
import random
import pandas as pd

from pyspark import SparkConf, SparkContext
%matplotlib inline
import matplotlib.pyplot as plt
import re


sc=SparkContext(appName = "MovieRatingGraph2")
lines = sc.textFile("/data/movie-ratings/ratings.dat")
data = lines.map(lambda x: re.split(r'::', x)) 
ratings = data.map(lambda x:float(x[2]))
users =ratings.map(lambda x: (x,1)) 
users_grouped = users.reduceByKey(lambda a,b: (a+b)) 

datasorted=users_grouped.sortByKey(ascending=True)

plt.title("Count per rating\n")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.hist(ratings.take(ratings.count()), align='mid', rwidth=0.9, bins=10)

#configure  X axes
plt.xlim(0,5)
plt.xticks([0.5,1,1.5,2,2.5,3,3.5,4,4.5,5])


plt.show()

datasorted=users_grouped.sortByKey(ascending=True)

usersgroupedX=datasorted.map(lambda x:(x[0]))
usersgroupedY=datasorted.map(lambda x:(x[1]))

plt.title("Count per rating\n")
plt.xlabel("Rating")
plt.ylabel("Count")

plt.scatter(usersgroupedX.take(ratings.count()), usersgroupedY.take(ratings.count()))
plt.show()




sc.stop()
